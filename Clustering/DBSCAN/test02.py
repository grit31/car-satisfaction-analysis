import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import DBSCAN
from sklearn.metrics import (
    silhouette_score, adjusted_rand_score,
    normalized_mutual_info_score, homogeneity_score,
    completeness_score, v_measure_score
)
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
# æ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
# ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
plt.rcParams['axes.unicode_minus'] = False
# 1. åŠ è½½æ•°æ®
file_path = "..\\..\\car+evaluation\\car.data"
column_names = ["buying", "maint", "doors", "persons", "lug_boot", "safety", "car_acceptability"]
df = pd.read_csv(file_path, names=column_names)

# 2. è‡ªå®šä¹‰è¯­ä¹‰ç¼–ç ï¼ˆä» 1 å¼€å§‹ï¼‰
map_buying = {'low': 1, 'med': 2, 'high': 3, 'vhigh': 4}
map_maint = {'low': 1, 'med': 2, 'high': 3, 'vhigh': 4}
map_doors = {'2': 1, '3': 2, '4': 3, '5more': 4}
map_persons = {'2': 1, '4': 2, 'more': 3}
map_lug_boot = {'small': 1, 'med': 2, 'big': 3}
map_safety = {'low': 1, 'med': 2, 'high': 3}
map_acceptability = {'unacc': 1, 'acc': 2, 'good': 3, 'vgood': 4}

X = df.drop(columns=["car_acceptability"]).copy()
X["buying"] = X["buying"].map(map_buying)
X["maint"] = X["maint"].map(map_maint)
X["doors"] = X["doors"].map(map_doors)
X["persons"] = X["persons"].map(map_persons)
X["lug_boot"] = X["lug_boot"].map(map_lug_boot)
X["safety"] = X["safety"].map(map_safety)
y_true = df["car_acceptability"].map(map_acceptability)

# 3. æ„å»º DBSCAN æ¨¡å‹
dbscan = DBSCAN(eps=0.8, min_samples=5, metric='euclidean')
y_pred = dbscan.fit_predict(X)

# 4. è·å–èšç±»æ•°ï¼ˆæ’é™¤å™ªå£°ï¼‰
n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)
print(f"âœ… DBSCAN èšå‡ºç°‡æ•°: {n_clusters}")

# 5. PCA é™ç»´
X_pca = PCA(n_components=2).fit_transform(X)

# 6. å¯è§†åŒ–å¯¹æ¯”
fig, axs = plt.subplots(1, 2, figsize=(12, 5))
axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, cmap='tab10', s=10)
axs[0].set_title('True Labels (PCA Projection)')
axs[0].set_xlabel('PC1')
axs[0].set_ylabel('PC2')
axs[0].grid(True)

axs[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='tab10', s=10)
axs[1].set_title(f'DBSCAN Clustering (k={n_clusters})')
axs[1].set_xlabel('PC1')
axs[1].set_ylabel('PC2')
axs[1].grid(True)

plt.tight_layout()
plt.savefig("dbscan_cluster_comparison.png", dpi=300)
plt.show()

# 7. èšç±»è¯„ä¼°å‡½æ•°
def evaluate_clustering(y_true, y_pred):
    try:
        silhouette = silhouette_score(X, y_pred) if len(set(y_pred)) > 1 else None
    except:
        silhouette = None
    return {
        "Silhouette": silhouette,
        "ARI": adjusted_rand_score(y_true, y_pred),
        "NMI": normalized_mutual_info_score(y_true, y_pred),
        "Homogeneity": homogeneity_score(y_true, y_pred),
        "Completeness": completeness_score(y_true, y_pred),
        "V-Measure": v_measure_score(y_true, y_pred)
    }

results = evaluate_clustering(y_true, y_pred)
comparison_df = pd.DataFrame([{
    "Kå€¼": n_clusters,
    "Silhouette": results["Silhouette"],
    "ARI": results["ARI"],
    "NMI": results["NMI"],
    "Homogeneity": results["Homogeneity"],
    "Completeness": results["Completeness"],
    "V-Measure": results["V-Measure"]
}])

# 8. ä¿å­˜èšç±»è¯„ä»·æŒ‡æ ‡
comparison_df.to_csv("dbscan_comparison_results.csv", index=False)
print("âœ… å·²ä¿å­˜èšç±»æ•ˆæœè¡¨ä¸º 'dbscan_comparison_results.csv'")
print(comparison_df.to_string(index=False))

# 9. æ¨¡æ‹Ÿâ€œèšç±»ä¸­å¿ƒâ€ï¼šä½¿ç”¨æ¯ç±»æ ·æœ¬å‡å€¼
df_clusters = X.copy()
df_clusters["cluster"] = y_pred
cluster_centers = df_clusters[df_clusters["cluster"] != -1].groupby("cluster").mean().reset_index(drop=True)

# 10. æ‰“å°æœªè§£ç çš„èšç±»â€œä¸­å¿ƒâ€
print("\nğŸ“Œ DBSCAN å„ç°‡å‡å€¼ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿä¸­å¿ƒ - ç¼–ç æ•°å€¼ï¼‰:")
print(cluster_centers.round(2).to_string(index=False))

# 11. è§£ç å‡½æ•°
def decode_centroids(centroids):
    reverse_maps = {
        'buying': {v: k for k, v in map_buying.items()},
        'maint': {v: k for k, v in map_maint.items()},
        'doors': {v: k for k, v in map_doors.items()},
        'persons': {v: k for k, v in map_persons.items()},
        'lug_boot': {v: k for k, v in map_lug_boot.items()},
        'safety': {v: k for k, v in map_safety.items()}
    }
    decoded = []
    for row in centroids.values:
        decoded_row = []
        for idx, col in enumerate(X.columns):
            val = int(round(row[idx]))
            decoded_val = reverse_maps[col].get(val, "(?)")
            decoded_row.append(decoded_val)
        decoded.append(decoded_row)
    return pd.DataFrame(decoded, columns=X.columns)

decoded_centroids = decode_centroids(cluster_centers)
print("\nğŸ“Œ DBSCAN å„ç°‡ä¸­å¿ƒï¼ˆè§£ç åï¼‰:")
print(decoded_centroids.to_string(index=False))

# 12. ä¿å­˜ä¸­å¿ƒå¯¹æ¯”ç»“æœ
decoded_centroids["èšç±»æ¨¡å‹"] = f"DBSCAN (k={n_clusters})"
decoded_centroids.to_csv("centroids_dbscan.csv", index=False, encoding='utf-8-sig')
print("âœ… å·²ä¿å­˜èšç±»ä¸­å¿ƒå¯¹æ¯”è¡¨ä¸º 'centroids_dbscan.csv'")

# 13. æ‰“å°æ¯ç±»æ ·æœ¬æ•°é‡ï¼ˆä¸å«å™ªå£°ï¼‰
print("\nğŸ“¦ æ¯ç±»æ ·æœ¬æ•°é‡ï¼ˆä¸å«å™ªå£°ï¼‰ï¼š")
cluster_counts = df_clusters[df_clusters["cluster"] != -1]["cluster"].value_counts().sort_index()
for i, count in cluster_counts.items():
    print(f"Cluster {i}: {count} ä¸ªæ ·æœ¬")

# 14. å¯è§†åŒ–æ ·æœ¬åˆ†å¸ƒæŸ±çŠ¶å›¾
fig, ax = plt.subplots(figsize=(7, 5))
ax.bar(cluster_counts.index, cluster_counts.values, color='mediumseagreen')
ax.set_title(f"DBSCAN èšç±»æ ·æœ¬åˆ†å¸ƒï¼ˆk={n_clusters}ï¼‰")
ax.set_xlabel("ç°‡ç¼–å·")
ax.set_ylabel("æ ·æœ¬æ•°é‡")
ax.set_xticks(cluster_counts.index)
for i, v in enumerate(cluster_counts.values):
    ax.text(cluster_counts.index[i], v + 5, str(v), ha='center')
plt.tight_layout()
plt.savefig("dbscan_cluster_distribution.png", dpi=300)
plt.show()

print("âœ… å·²ä¿å­˜èšç±»æ ·æœ¬åˆ†å¸ƒæŸ±çŠ¶å›¾ä¸º 'dbscan_cluster_distribution.png'")
